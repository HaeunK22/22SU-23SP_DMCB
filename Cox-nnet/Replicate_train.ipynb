{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from Code_manual import Coxnnet, PartialNLL\n",
    "from lifelines.utils import concordance_index\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Survival Analysis\\\\Cox-nnet'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load KIRC data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "expr_data = pd.read_table(\"./KIRC_expr.tsv\", header=None)\n",
    "\n",
    "time_data = pd.read_table(\"./KIRC_time.tsv\", header=None)\n",
    "\n",
    "observed_data = pd.read_table(\"./KIRC_event.tsv\", header=None)\n",
    "\n",
    "map_dict = {'DECEASED' : 1,\n",
    "            'LIVING' : 0}\n",
    "observed_data = observed_data[0].apply(lambda x : map_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor(np.array(expr_data), dtype=torch.float)\n",
    "time = torch.tensor(time_data.to_numpy(), dtype=torch.long)\n",
    "observed = torch.tensor(observed_data.to_numpy(), dtype=torch.float)\n",
    "\n",
    "train_idx, test_idx, _, _ = train_test_split(np.arange(X.shape[0]), observed, test_size=0.3, random_state=42)\n",
    "\n",
    "train_X = X[train_idx,:]\n",
    "test_X = X[test_idx,:]\n",
    "\n",
    "train_time = time[train_idx]\n",
    "test_time = time[test_idx]\n",
    "\n",
    "train_observed = observed[train_idx]\n",
    "test_observed = observed[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = True if torch.cuda.is_available() else False\n",
    "if cuda:\n",
    "    device = torch.device(f'cuda:{0}' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dmcb\\AppData\\Local\\Temp\\ipykernel_8408\\655114107.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.x_data = torch.tensor(train_X, device=device)\n",
      "C:\\Users\\dmcb\\AppData\\Local\\Temp\\ipykernel_8408\\655114107.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.time_data = torch.tensor(train_time, device=device)\n",
      "C:\\Users\\dmcb\\AppData\\Local\\Temp\\ipykernel_8408\\655114107.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.observed_data = torch.tensor(train_observed, device=device)\n",
      "C:\\Users\\dmcb\\AppData\\Local\\Temp\\ipykernel_8408\\655114107.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.x_data = torch.tensor(test_X, device=device)\n",
      "C:\\Users\\dmcb\\AppData\\Local\\Temp\\ipykernel_8408\\655114107.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.time_data = torch.tensor(test_time, device=device)\n",
      "C:\\Users\\dmcb\\AppData\\Local\\Temp\\ipykernel_8408\\655114107.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.observed_data = torch.tensor(test_observed, device=device)\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.x_data = torch.tensor(train_X, device=device)\n",
    "        self.time_data = torch.tensor(train_time, device=device)\n",
    "        self.observed_data = torch.tensor(train_observed, device=device)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        x = torch.tensor(self.x_data[index]).clone().detach()\n",
    "        time = torch.tensor(self.time_data[index]).clone().detach()\n",
    "        observed = torch.tensor(self.observed_data[index]).clone().detach()\n",
    "        return x, time, observed\n",
    "    \n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.x_data = torch.tensor(test_X, device=device)\n",
    "        self.time_data = torch.tensor(test_time, device=device)\n",
    "        self.observed_data = torch.tensor(test_observed, device=device)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        x = torch.tensor(self.x_data[index]).clone().detach()\n",
    "        time = torch.tensor(self.time_data[index]).clone().detach()\n",
    "        observed = torch.tensor(self.observed_data[index]).clone().detach()\n",
    "        return x, time, observed\n",
    "    \n",
    "training_data = TrainDataset()\n",
    "test_data = TestDataset()\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size = training_data.__len__())\n",
    "test_dataloader = DataLoader(test_data, batch_size = test_data.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = []\n",
    "\n",
    "hidden_dim = 500\n",
    "model = Coxnnet(train_X.shape[1], hidden_dim)\n",
    "model = model.to(device)\n",
    "learning_rate = 3e-5\n",
    "epochs = 100\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = PartialNLL()\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer, t):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, time, observed) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, time, observed)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()   # bc/ gradients by default add up; to prevent double counting.\n",
    "        loss.backward() # deposits gradients\n",
    "        optimizer.step()    # adjust parameters with gradients\n",
    "        \n",
    "        train_perf = 0\n",
    "        train_perf = concordance_index(event_times = time.cpu().detach().numpy(),\n",
    "                                       event_observed = observed.cpu().detach().numpy(),\n",
    "                                       predicted_scores = -pred.cpu().detach().numpy())\n",
    "        \n",
    "        if ((t+1) % 25 == 0):\n",
    "            print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "            print(f\"Train Accuracy: {train_perf}\\n\")\n",
    "\n",
    "# iterate over test dataset to check model performance\n",
    "def test_loop(dataloader, model, t):\n",
    "    size = len(dataloader.dataset)\n",
    "    test_perf = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, time, observed in dataloader:\n",
    "            pred = model.forward(X)\n",
    "            test_perf = concordance_index(event_times = time.cpu().detach().numpy(),\n",
    "                                          event_observed = observed.cpu().detach().numpy(),\n",
    "                                          predicted_scores = -pred.cpu().detach().numpy())\n",
    "\n",
    "    if ((t+1) % 25 == 0):\n",
    "        print(f\"Test Accuracy: {test_perf}\\n\")\n",
    "    if (t == epochs - 1):\n",
    "        acc.append(float(test_perf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------\n",
      "Replicate 1\n",
      "------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dmcb\\AppData\\Local\\Temp\\ipykernel_8408\\655114107.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(self.x_data[index]).clone().detach()\n",
      "C:\\Users\\dmcb\\AppData\\Local\\Temp\\ipykernel_8408\\655114107.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  time = torch.tensor(self.time_data[index]).clone().detach()\n",
      "C:\\Users\\dmcb\\AppData\\Local\\Temp\\ipykernel_8408\\655114107.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  observed = torch.tensor(self.observed_data[index]).clone().detach()\n",
      "C:\\Users\\dmcb\\AppData\\Local\\Temp\\ipykernel_8408\\655114107.py:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(self.x_data[index]).clone().detach()\n",
      "C:\\Users\\dmcb\\AppData\\Local\\Temp\\ipykernel_8408\\655114107.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  time = torch.tensor(self.time_data[index]).clone().detach()\n",
      "C:\\Users\\dmcb\\AppData\\Local\\Temp\\ipykernel_8408\\655114107.py:34: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  observed = torch.tensor(self.observed_data[index]).clone().detach()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25\n",
      "-------------------------------\n",
      "Train Accuracy: 0.7265781649726889\n",
      "\n",
      "Test Accuracy: 0.534655489674913\n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Train Accuracy: 0.7538200926502109\n",
      "\n",
      "Test Accuracy: 0.604784297689634\n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "Train Accuracy: 0.7925741547396805\n",
      "\n",
      "Test Accuracy: 0.6107135555101206\n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Train Accuracy: 0.8121067551683606\n",
      "\n",
      "Test Accuracy: 0.6027397260273972\n",
      "\n",
      "------------\n",
      "Replicate 2\n",
      "------------\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Train Accuracy: 0.7212542349443407\n",
      "\n",
      "Test Accuracy: 0.6219586996524228\n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Train Accuracy: 0.7522298278365485\n",
      "\n",
      "Test Accuracy: 0.5317930893477817\n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "Train Accuracy: 0.7987969300974902\n",
      "\n",
      "Test Accuracy: 0.595379268043345\n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Train Accuracy: 0.8369632856253889\n",
      "\n",
      "Test Accuracy: 0.5540789204661624\n",
      "\n",
      "------------\n",
      "Replicate 3\n",
      "------------\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Train Accuracy: 0.7153425983544216\n",
      "\n",
      "Test Accuracy: 0.6101001840114496\n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Train Accuracy: 0.7488764433381733\n",
      "\n",
      "Test Accuracy: 0.5865876098957269\n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "Train Accuracy: 0.7833782755998064\n",
      "\n",
      "Test Accuracy: 0.6219586996524228\n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Train Accuracy: 0.7761529419899053\n",
      "\n",
      "Test Accuracy: 0.5465140053158863\n",
      "\n",
      "------------\n",
      "Replicate 4\n",
      "------------\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Train Accuracy: 0.6858535573532463\n",
      "\n",
      "Test Accuracy: 0.5894500102228583\n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Train Accuracy: 0.7812694461729932\n",
      "\n",
      "Test Accuracy: 0.5685953792680434\n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "Train Accuracy: 0.7881836410150038\n",
      "\n",
      "Test Accuracy: 0.5984461255367001\n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Train Accuracy: 0.801977459724815\n",
      "\n",
      "Test Accuracy: 0.6358617869556328\n",
      "\n",
      "------------\n",
      "Replicate 5\n",
      "------------\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Train Accuracy: 0.7042107446587845\n",
      "\n",
      "Test Accuracy: 0.5935391535473319\n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Train Accuracy: 0.7848648274908387\n",
      "\n",
      "Test Accuracy: 0.5861786955632795\n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "Train Accuracy: 0.788909631473415\n",
      "\n",
      "Test Accuracy: 0.6080556123492128\n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Train Accuracy: 0.7987623591232801\n",
      "\n",
      "Test Accuracy: 0.6403598446125537\n",
      "\n",
      "------------\n",
      "Replicate 6\n",
      "------------\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Train Accuracy: 0.7081172647445205\n",
      "\n",
      "Test Accuracy: 0.5777959517481087\n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Train Accuracy: 0.7815460139666736\n",
      "\n",
      "Test Accuracy: 0.6072377836843181\n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "Train Accuracy: 0.8005600497822029\n",
      "\n",
      "Test Accuracy: 0.5919034962175425\n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Train Accuracy: 0.8257968609555417\n",
      "\n",
      "Test Accuracy: 0.5782048660805561\n",
      "\n",
      "------------\n",
      "Replicate 7\n",
      "------------\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Train Accuracy: 0.7640185300421766\n",
      "\n",
      "Test Accuracy: 0.609282355346555\n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Train Accuracy: 0.7943027034501833\n",
      "\n",
      "Test Accuracy: 0.57452463708853\n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "Train Accuracy: 0.7835511304708567\n",
      "\n",
      "Test Accuracy: 0.6285013289715805\n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Train Accuracy: 0.8105510613289082\n",
      "\n",
      "Test Accuracy: 0.6090778981803312\n",
      "\n",
      "------------\n",
      "Replicate 8\n",
      "------------\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Train Accuracy: 0.7342874922215308\n",
      "\n",
      "Test Accuracy: 0.6469024739317113\n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Train Accuracy: 0.7521606858881283\n",
      "\n",
      "Test Accuracy: 0.5784093232467798\n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "Train Accuracy: 0.7728686994399502\n",
      "\n",
      "Test Accuracy: 0.6154160703332652\n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Train Accuracy: 0.8088916545668257\n",
      "\n",
      "Test Accuracy: 0.5892455530566346\n",
      "\n",
      "------------\n",
      "Replicate 9\n",
      "------------\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Train Accuracy: 0.7175897116780751\n",
      "\n",
      "Test Accuracy: 0.5775914945818851\n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Train Accuracy: 0.7535435248565304\n",
      "\n",
      "Test Accuracy: 0.6197096708239623\n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "Train Accuracy: 0.7879416441955335\n",
      "\n",
      "Test Accuracy: 0.581476180740135\n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Train Accuracy: 0.8221669086634862\n",
      "\n",
      "Test Accuracy: 0.603557554692292\n",
      "\n",
      "------------\n",
      "Replicate 10\n",
      "------------\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Train Accuracy: 0.7230173546290535\n",
      "\n",
      "Test Accuracy: 0.5553056634635044\n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Train Accuracy: 0.800629191730623\n",
      "\n",
      "Test Accuracy: 0.583316295236148\n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "Train Accuracy: 0.8042937149968886\n",
      "\n",
      "Test Accuracy: 0.5790226947454509\n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Train Accuracy: 0.8272488418723639\n",
      "\n",
      "Test Accuracy: 0.5855653240646085\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(f\"------------\\nReplicate {i+1}\\n------------\")\n",
    "    train_idx, test_idx, _, _ = train_test_split(np.arange(X.shape[0]), observed, test_size=0.3, random_state=i)\n",
    "\n",
    "    train_X = X[train_idx,:]\n",
    "    test_X = X[test_idx,:]\n",
    "\n",
    "    train_time = time[train_idx]\n",
    "    test_time = time[test_idx]\n",
    "\n",
    "    train_observed = observed[train_idx]\n",
    "    test_observed = observed[test_idx]\n",
    "    model = Coxnnet(train_X.shape[1], hidden_dim)\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for t in range(epochs):\n",
    "        train_loop(train_dataloader, model, loss_fn, optimizer, t)\n",
    "        test_loop(test_dataloader, model, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.583929666734819, 0.5798405234103455, 0.6037620118585156, 0.6260478429768963, 0.5348599468411368, 0.6297280719689226, 0.6078511551829892, 0.5685953792680434, 0.6567164179104478, 0.617460641995502]\n"
     ]
    }
   ],
   "source": [
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KIRC_acc = acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load LAML data & train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expr_data = pd.read_table(\"./LAML_expr.tsv\", header=None)\n",
    "\n",
    "time_data = pd.read_table(\"./LAML_time.tsv\", header=None)\n",
    "\n",
    "observed_data = pd.read_table(\"./LAML_event.tsv\", header=None)\n",
    "\n",
    "map_dict = {'DECEASED' : 1,\n",
    "            'LIVING' : 0}\n",
    "observed_data = observed_data[0].apply(lambda x : map_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------\n",
      "Replicate 1\n",
      "------------\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Train Accuracy: 0.6914540551752748\n",
      "\n",
      "Test Accuracy: 0.5266816601921898\n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Train Accuracy: 0.7529212473207495\n",
      "\n",
      "Test Accuracy: 0.5395624616642813\n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "Train Accuracy: 0.7995920625043214\n",
      "\n",
      "Test Accuracy: 0.6158249846657126\n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Train Accuracy: 0.7812003042245731\n",
      "\n",
      "Test Accuracy: 0.6311592721324882\n",
      "\n",
      "------------\n",
      "Replicate 2\n",
      "------------\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Train Accuracy: 0.7019636313351311\n",
      "\n",
      "Test Accuracy: 0.5667552647720303\n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Train Accuracy: 0.7637765332227062\n",
      "\n",
      "Test Accuracy: 0.5655285217746882\n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "Train Accuracy: 0.8044319988937289\n",
      "\n",
      "Test Accuracy: 0.6062154978531997\n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Train Accuracy: 0.8329876235912328\n",
      "\n",
      "Test Accuracy: 0.5865876098957269\n",
      "\n",
      "------------\n",
      "Replicate 3\n",
      "------------\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Train Accuracy: 0.7351863375509922\n",
      "\n",
      "Test Accuracy: 0.5708444080965038\n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Train Accuracy: 0.7838276982645371\n",
      "\n",
      "Test Accuracy: 0.6143937845021468\n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "Train Accuracy: 0.7698264537094656\n",
      "\n",
      "Test Accuracy: 0.6123492128399101\n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Train Accuracy: 0.8138007329046533\n",
      "\n",
      "Test Accuracy: 0.569413207932938\n",
      "\n",
      "------------\n",
      "Replicate 4\n",
      "------------\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Train Accuracy: 0.7202862476664592\n",
      "\n",
      "Test Accuracy: 0.5683909221018196\n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Train Accuracy: 0.7451773490976976\n",
      "\n",
      "Test Accuracy: 0.5240237170312819\n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "Train Accuracy: 0.7880453571181636\n",
      "\n",
      "Test Accuracy: 0.6207319566550807\n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Train Accuracy: 0.8156675655119962\n",
      "\n",
      "Test Accuracy: 0.5976282968718054\n",
      "\n",
      "------------\n",
      "Replicate 5\n",
      "------------\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Train Accuracy: 0.7624282652285141\n",
      "\n",
      "Test Accuracy: 0.5925168677162135\n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Train Accuracy: 0.7820300076056144\n",
      "\n",
      "Test Accuracy: 0.6043753833571867\n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "Train Accuracy: 0.7795408974624904\n",
      "\n",
      "Test Accuracy: 0.5818850950725823\n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Train Accuracy: 0.8093756482057665\n",
      "\n",
      "Test Accuracy: 0.6413821304436721\n",
      "\n",
      "------------\n",
      "Replicate 6\n",
      "------------\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Train Accuracy: 0.714927746663901\n",
      "\n",
      "Test Accuracy: 0.6094868125127786\n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Train Accuracy: 0.7552720735670331\n",
      "\n",
      "Test Accuracy: 0.5912901247188714\n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "Train Accuracy: 0.8062642605268616\n",
      "\n",
      "Test Accuracy: 0.590881210386424\n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Train Accuracy: 0.8050542764295098\n",
      "\n",
      "Test Accuracy: 0.6176650991617256\n",
      "\n",
      "------------\n",
      "Replicate 7\n",
      "------------\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Train Accuracy: 0.6825001728548711\n",
      "\n",
      "Test Accuracy: 0.5645062359435699\n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Train Accuracy: 0.7749083869183434\n",
      "\n",
      "Test Accuracy: 0.5393580044980577\n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "Train Accuracy: 0.785037682361889\n",
      "\n",
      "Test Accuracy: 0.6060110406869761\n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Train Accuracy: 0.8384152665422112\n",
      "\n",
      "Test Accuracy: 0.617460641995502\n",
      "\n",
      "------------\n",
      "Replicate 8\n",
      "------------\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Train Accuracy: 0.7302081172647445\n",
      "\n",
      "Test Accuracy: 0.5935391535473319\n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Train Accuracy: 0.7683399018184333\n",
      "\n",
      "Test Accuracy: 0.617460641995502\n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "Train Accuracy: 0.8104819193804882\n",
      "\n",
      "Test Accuracy: 0.6507871600899612\n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Train Accuracy: 0.8245868768581899\n",
      "\n",
      "Test Accuracy: 0.6221631568186465\n",
      "\n",
      "------------\n",
      "Replicate 9\n",
      "------------\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Train Accuracy: 0.731210675516836\n",
      "\n",
      "Test Accuracy: 0.5620527499488857\n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Train Accuracy: 0.7648136624490078\n",
      "\n",
      "Test Accuracy: 0.5598037211204253\n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "Train Accuracy: 0.7675793403858121\n",
      "\n",
      "Test Accuracy: 0.6620323042322633\n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Train Accuracy: 0.8245868768581899\n",
      "\n",
      "Test Accuracy: 0.6438356164383562\n",
      "\n",
      "------------\n",
      "Replicate 10\n",
      "------------\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Train Accuracy: 0.703277328355113\n",
      "\n",
      "Test Accuracy: 0.5777959517481087\n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Train Accuracy: 0.7433105164903547\n",
      "\n",
      "Test Accuracy: 0.5794316090778981\n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "Train Accuracy: 0.7722118509299593\n",
      "\n",
      "Test Accuracy: 0.5845430382334901\n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Train Accuracy: 0.7927124386365207\n",
      "\n",
      "Test Accuracy: 0.5465140053158863\n",
      "\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "\n",
    "for i in range(10):\n",
    "    print(f\"------------\\nReplicate {i+1}\\n------------\")\n",
    "    train_idx, test_idx, _, _ = train_test_split(np.arange(X.shape[0]), observed, test_size=0.3, random_state=i)\n",
    "\n",
    "    train_X = X[train_idx,:]\n",
    "    test_X = X[test_idx,:]\n",
    "\n",
    "    train_time = time[train_idx]\n",
    "    test_time = time[test_idx]\n",
    "\n",
    "    train_observed = observed[train_idx]\n",
    "    test_observed = observed[test_idx]\n",
    "    model = Coxnnet(train_X.shape[1], hidden_dim)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for t in range(epochs):\n",
    "        train_loop(train_dataloader, model, loss_fn, optimizer, t)\n",
    "        test_loop(test_dataloader, model, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAML_acc = acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load PAAD data & train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expr_data = pd.read_table(\"./PAAD_expr.tsv\", header=None)\n",
    "\n",
    "time_data = pd.read_table(\"./PAAD_time.tsv\", header=None)\n",
    "\n",
    "observed_data = pd.read_table(\"./PAAD_event.tsv\", header=None)\n",
    "\n",
    "map_dict = {'DECEASED' : 1,\n",
    "            'LIVING' : 0}\n",
    "observed_data = observed_data[0].apply(lambda x : map_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------\n",
      "Replicate 1\n",
      "------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Survival Analysis\\Cox-nnet\\Replicate_train.ipynb 셀 18\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Survival%20Analysis/Cox-nnet/Replicate_train.ipynb#X31sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m         train_loop(train_dataloader, model, loss_fn, optimizer, t)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Survival%20Analysis/Cox-nnet/Replicate_train.ipynb#X31sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m         model\u001b[39m.\u001b[39meval()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Survival%20Analysis/Cox-nnet/Replicate_train.ipynb#X31sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m         test_loop(test_dataloader, model, t)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Survival%20Analysis/Cox-nnet/Replicate_train.ipynb#X31sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m PAAD_acc \u001b[39m=\u001b[39m acc\n",
      "\u001b[1;32md:\\Survival Analysis\\Cox-nnet\\Replicate_train.ipynb 셀 18\u001b[0m in \u001b[0;36mtest_loop\u001b[1;34m(dataloader, model, t)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Survival%20Analysis/Cox-nnet/Replicate_train.ipynb#X31sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m test_perf \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Survival%20Analysis/Cox-nnet/Replicate_train.ipynb#X31sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Survival%20Analysis/Cox-nnet/Replicate_train.ipynb#X31sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     \u001b[39mfor\u001b[39;00m X, time, observed \u001b[39min\u001b[39;00m dataloader:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Survival%20Analysis/Cox-nnet/Replicate_train.ipynb#X31sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m         pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mforward(X)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Survival%20Analysis/Cox-nnet/Replicate_train.ipynb#X31sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m         test_perf \u001b[39m=\u001b[39m concordance_index(event_times \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mnumpy(),\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Survival%20Analysis/Cox-nnet/Replicate_train.ipynb#X31sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m                                       event_observed \u001b[39m=\u001b[39m observed\u001b[39m.\u001b[39mnumpy(),\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Survival%20Analysis/Cox-nnet/Replicate_train.ipynb#X31sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m                                       predicted_scores \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mpred\u001b[39m.\u001b[39mnumpy())\n",
      "File \u001b[1;32mc:\\Users\\dmcb\\anaconda3\\envs\\hek\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:652\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    649\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    650\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    651\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 652\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    653\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    654\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    655\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    656\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\dmcb\\anaconda3\\envs\\hek\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:692\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    691\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 692\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    693\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    694\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\dmcb\\anaconda3\\envs\\hek\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\dmcb\\anaconda3\\envs\\hek\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "\u001b[1;32md:\\Survival Analysis\\Cox-nnet\\Replicate_train.ipynb 셀 18\u001b[0m in \u001b[0;36mTestDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Survival%20Analysis/Cox-nnet/Replicate_train.ipynb#X31sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, index):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Survival%20Analysis/Cox-nnet/Replicate_train.ipynb#X31sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mx_data[index])\u001b[39m.\u001b[39mclone()\u001b[39m.\u001b[39mdetach()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Survival%20Analysis/Cox-nnet/Replicate_train.ipynb#X31sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     time \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtime_data[index])\u001b[39m.\u001b[39mclone()\u001b[39m.\u001b[39mdetach()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Survival%20Analysis/Cox-nnet/Replicate_train.ipynb#X31sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     observed \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobserved_data[index])\u001b[39m.\u001b[39mclone()\u001b[39m.\u001b[39mdetach()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "\n",
    "for i in range(10):\n",
    "    print(f\"------------\\nReplicate {i+1}\\n------------\")\n",
    "    train_idx, test_idx, _, _ = train_test_split(np.arange(X.shape[0]), observed, test_size=0.3, random_state=i)\n",
    "\n",
    "    train_X = X[train_idx,:]\n",
    "    test_X = X[test_idx,:]\n",
    "\n",
    "    train_time = time[train_idx]\n",
    "    test_time = time[test_idx]\n",
    "\n",
    "    train_observed = observed[train_idx]\n",
    "    test_observed = observed[test_idx]\n",
    "    model = Coxnnet(train_X.shape[1], hidden_dim)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for t in range(epochs):\n",
    "        model.train()\n",
    "        train_loop(train_dataloader, model, loss_fn, optimizer, t)\n",
    "        model.eval()\n",
    "        test_loop(test_dataloader, model, t)\n",
    "        \n",
    "PAAD_acc = acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average of accuracy per data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average of KIRC acc : 0.6008791658147619\n",
      "Average of LAML acc : 0.6073809037006749\n",
      "Average of PAAD acc : 0.6154160703332651\n"
     ]
    }
   ],
   "source": [
    "print(f\"Average of KIRC acc : {sum(KIRC_acc)/len(KIRC_acc)}\")\n",
    "print(f\"Average of LAML acc : {sum(LAML_acc)/len(LAML_acc)}\")\n",
    "print(f\"Average of PAAD acc : {sum(PAAD_acc)/len(PAAD_acc)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x960 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "figure(figsize=(8, 12), dpi=80)\n",
    "\n",
    "data = [KIRC_acc, LAML_acc, PAAD_acc]\n",
    "plt.boxplot(data, labels=['KIRC', 'LAML', 'PAAD'])\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('hek')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0491cbb347169b674faecc57483f555701d022daf991287b1e1cb5e40250a57a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
